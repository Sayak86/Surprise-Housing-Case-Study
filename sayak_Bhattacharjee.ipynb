{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House Price Prediction using Linear, Ridge and Lasso Regression\n",
    "---\n",
    "The solution is divided into the following sections: \n",
    "- Data understanding\n",
    "- Data cleaning\n",
    "- Data Exploration  \n",
    "    1. Univariate Analysis\n",
    "    2. Bivariate Analysis\n",
    "    3. Outliers treatment\n",
    "- Data preparation\n",
    "- Model building and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import os\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "house = pd.read_csv('train.csv',na_values='NA')\n",
    "house.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the data\n",
    "house.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type of the columns\n",
    "house.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starts Data cleaning\n",
    "# Verification of null columns - Loop over all the columns and create an object with null columns only where the null count>50\n",
    "\n",
    "null_col_dict={}\n",
    "\n",
    "for i in house.columns:\n",
    "    if house[i].isna().sum()>50:\n",
    "        null_col_dict.update({i:house[i].isna().sum()})\n",
    "null_col_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now we will only drop these columns from our data set\n",
    "list(null_col_dict.keys())\n",
    "\n",
    "house.drop(columns=list(null_col_dict.keys()),inplace=True,axis=1)\n",
    "house.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding duplicate rows\n",
    "\n",
    "duplicates = house.duplicated().sum()\n",
    "duplicates\n",
    "\n",
    "# So no duplicate rows are found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for empty rows\n",
    "house[house.isnull().all(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should drop the ID column\n",
    "house.drop(columns=['Id'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.select_dtypes(include= ['float64','int64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the below fields to object data type as they carry  categorical data\n",
    ">   1. MSSubClass\n",
    ">   2. OverallQual\n",
    ">   3. OverallCond\n",
    "\n",
    "### Change the below fields to numeric data type as they carry  continoius data\n",
    ">   1. MasVnrArea\n",
    "\n",
    "### delete below columns as the data variation is negligigble\n",
    ">   1. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert int to object\n",
    "\n",
    "house[ ['MSSubClass','OverallQual','OverallCond'] ] = house[ ['MSSubClass','OverallQual','OverallCond'] ].astype('object')\n",
    "house.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data type to int\n",
    "\n",
    "house['MasVnrArea'] = pd.to_numeric(house['MasVnrArea'], errors='coerce')\n",
    "house.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete column Utilities as it has only one data of different category, rest all are of the same category\n",
    "house.drop(columns=['Utilities'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistisal description of our data set\n",
    "house.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing still reamining null values\n",
    "   - For Continious fields = null can be replaced by meadian\n",
    "   - For categorical fields \n",
    "       > .If mode is predominent then replace by mode.\n",
    "       > .If not then replace by new category 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_col_dict={}\n",
    "\n",
    "for i in house.columns:\n",
    "    if house[i].isna().sum()>0:\n",
    "        null_col_dict.update({i:house[i].isna().sum()})\n",
    "null_col_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MasVnrArea - continious variable, will impute with the median value. As we can see presence of outliers fro the decribe() step\n",
    "house.MasVnrArea = house.MasVnrArea.fillna(house.MasVnrArea.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical field Null handling - BsmtQual\n",
    "print(house.BsmtQual.value_counts())\n",
    "print(house.BsmtQual.mode()[0])\n",
    "\n",
    "# With such distribution we cant impute with mode and this we will introduce 'Unknown' for the missing values\n",
    "house.BsmtQual = house.BsmtQual.fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Categorical field Null handling - BsmtCond\n",
    "print(house.BsmtCond.value_counts())\n",
    "print(house.BsmtCond.mode()[0])\n",
    "\n",
    "# As we see the mode value 'TA' compprehensively larger than any other values, we will impute with the mode here\n",
    "house.BsmtCond = house.BsmtCond.fillna(house.BsmtCond.mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_col_dict={}\n",
    "\n",
    "for i in house.columns:\n",
    "    if house[i].isna().sum()>0:\n",
    "        null_col_dict.update({i:house[i].isna().sum()})\n",
    "null_col_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Categorical field Null handling - BsmtExposure\n",
    "sns.displot(house['BsmtExposure'])\n",
    "\n",
    "# we can impute nulls with mode as the mode is significantly higher than the rest\n",
    "house.BsmtExposure = house.BsmtExposure.fillna(house.BsmtExposure.mode()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Categorical field Null handling - BsmtFinType1\n",
    "sns.displot(house['BsmtFinType1'])\n",
    "\n",
    "# With such distribution we cant impute with mode and this we will introduce 'Unknown' for the missing values\n",
    "house.BsmtFinType1 = house.BsmtFinType1.fillna('Unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Categorical field Null handling - BsmtFinType2\n",
    "sns.displot(house['BsmtFinType2'])\n",
    "\n",
    "# we can impute nulls with mode as the mode is significantly higher than the rest\n",
    "house.BsmtFinType2 = house.BsmtFinType2.fillna(house.BsmtFinType2.mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Categorical field Null handling - BsmtFinType2\n",
    "sns.displot(house['Electrical'])\n",
    "\n",
    "# we can impute nulls with mode as the mode is significantly higher than the rest\n",
    "house.Electrical = house.Electrical.fillna(house.Electrical.mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Analysis - Target Variable SalePrice\n",
    "house.SalePrice.describe()\n",
    "sns.boxplot(house.SalePrice)\n",
    "\n",
    "sns.displot(house.SalePrice,kind='kde')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can find that because of certain outliers the target variable is right skewed. I come to know from my own study that right skewness can be handled by various means, some of them are\n",
    "- Log Transformation\n",
    "- Square Root Transformation    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the degree of skew\n",
    "house.SalePrice.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check Square root transformation first and verify \n",
    "x =np.sqrt(house.SalePrice).skew()\n",
    "print(x)\n",
    "# Lets apply Log transformation now\n",
    "y = np.log(house.SalePrice).skew()\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skew value as close to 0 is considered better distribution. Hence we would select the log transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.SalePrice = np.log(house.SalePrice)\n",
    "sns.displot(house.SalePrice,kind='kde')\n",
    "\n",
    "# Now we see our data is well distributed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis of continious variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_cont = house.select_dtypes(include=['int64', 'float64'])\n",
    "house_cont.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in house_cont.columns:\n",
    "    plt.figure(figsize=(15,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(col)\n",
    "    sns.histplot(house_cont[col],kde=True)\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.boxplot(house_cont[col])\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we can see certain outliers and non-normal distribution for most of the independent numerical variables.Although normal distribution of the independent variable is not a mandatory pre-requisite for inear regression but more the normality better can be the model. Outliers handling is required here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After performing IQR based elimination of outliers found that almost 50% of data got removed. Hence decided to replace outliers with with the data \n",
    "#of 5 percentile and 95 percentile\n",
    "\n",
    "for col in house_cont.columns:\n",
    "    if col != 'SalePrice':\n",
    "        house[col][house[col] <= house[col].quantile(0.05)] = house[col].quantile(0.05)\n",
    "        house[col][house[col] >= house[col].quantile(0.95)] = house[col].quantile(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once more plot the data after removing imputing outliers\n",
    "\n",
    "for col in house_cont.columns:\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(col)\n",
    "    sns.histplot(house[col],kde=True)\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.boxplot(house[col])\n",
    "\n",
    "# From the boxplots now we see better outliers state after handlig.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_cat = house.select_dtypes('object')\n",
    "house_cat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check data distribution for each of the variables\n",
    "\n",
    "i=0\n",
    "for col in house_cat.columns:\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(col)\n",
    "    plt.xticks(rotation=90)\n",
    "    sns.histplot(house[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above histograms we find certain categorical variables are highly skewed with very low variance in terms of data distribution. Such fields may have not be good predictor variables, so we can delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_col_del = []\n",
    "house_cat = house.select_dtypes('object')\n",
    "for col in house_cat.columns:\n",
    "    if (house_cat[col].value_counts()/house_cat.shape[0] >=.95).any():\n",
    "        house_col_del.append(col)\n",
    "print(house_col_del)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop the columns with more than 95% of the data in one category\n",
    "house.drop(columns=house_col_del,axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis of the continious variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.heatmap(house_cont.corr(), annot=True, fmt='.1f', cmap='coolwarm')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we can certainly find some co-related numerical fields like\n",
    ">   1. `GarageForCars` are highly co-related wih `GarageArea` (.9)\n",
    ">   2. `TotalBasementSF` are highly co-related wih `1stFlrSF` (.8)\n",
    ">   2. `GrLivArea` are highly co-related wih `TotRmsAbvGrd` (.8)\n",
    "\n",
    "#### We will drop the columns like `GarageForCars,1stFlrSF,TotRmsAbvGrd` as the corel is >.8. Rest can be evaluated by Lasso and VIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns because of the high co-linearity\n",
    "house.drop(columns=['GarageArea','TotRmsAbvGrd','1stFlrSF'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets have a look into the SalePrice vs Other numerical variable plotting. This should give us an indication about the linearity between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_cont = house.select_dtypes(include=['int64', 'float64'])\n",
    "for col in house_cont.columns:\n",
    "    plt.figure(figsize=(10,3))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(col)\n",
    "    sns.scatterplot(x=house[col], y=house['SalePrice'])\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We find that the below columns have only one value, hence we can drop them\n",
    ">   1. LowQualFinSF\n",
    ">   2. BedroomAbvGr\n",
    ">   3. KitchenAbvGr\n",
    ">   4. 3SsnPorch\n",
    ">   5. PoolArea\n",
    ">   6. MiscVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns because of low vaiance\n",
    "house.drop(columns=['BsmtFinSF2','LowQualFinSF','BsmtHalfBath','KitchenAbvGr','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate analysis of the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_cat = house.select_dtypes(['object'])\n",
    "for col in house_cat.columns:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(col)\n",
    "    plt.xticks(rotation=90)\n",
    "    sns.boxplot(x=house[col], y=house['SalePrice'].sort_values())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once more check for any null values\n",
    "null_col_dict={}\n",
    "for i in house.columns:\n",
    "    if house[i].isna().sum()>0:\n",
    "        null_col_dict.update({i:house[i].isna().sum()})\n",
    "null_col_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation - Here we will go through the categorical variables and start encoding them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for the ordinal columns\n",
    "\n",
    "house['LotShape'] = house['LotShape'].map({'IR1':0,'IR2':1,'IR3':2,'Reg':3})\n",
    "house['LandSlope'] = house['LandSlope'].map({'Gtl':0,'Mod':1,'Sev':2})\n",
    "house['HouseStyle'] = house['HouseStyle'].map({'1Story':0, '1.5Unf':1, '1.5Fin':2,  '2Story' :3, '2.5Unf':4, '2.5Fin':5, 'SFoyer':6, 'SLvl':7})\n",
    "house['ExterQual'] = house['ExterQual'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\n",
    "house['ExterCond'] = house['ExterCond'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\n",
    "house['BsmtQual'] = house['BsmtQual'].map({'Unknown':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\n",
    "house['BsmtCond'] = house['BsmtCond'].map({'Unknown':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\n",
    "house['BsmtExposure'] = house['BsmtExposure'].map({'Unknown':0,'No':1,'Mn':2,'Av':3,'Gd':4})\n",
    "house['BsmtFinType1'] = house['BsmtFinType1'].map({'Unknown':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\n",
    "house['BsmtFinType2'] = house['BsmtFinType2'].map({'Unknown':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\n",
    "house['HeatingQC'] = house['HeatingQC'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\n",
    "house['CentralAir'] = house['CentralAir'].map({'N':0,'Y':1})\n",
    "house['KitchenQual'] = house['KitchenQual'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\n",
    "house['Functional'] = house['Functional'].map({'Typ':0, 'Min1':1, 'Min2':2, 'Mod':3, 'Maj1':4, 'Maj2':5, 'Sev':6, 'Sal':7})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once more check for any null values\n",
    "null_col_dict={}\n",
    "for i in house.columns:\n",
    "    if house[i].isna().sum()>0:\n",
    "        null_col_dict.update({i:house[i].isna().sum()})\n",
    "null_col_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of the nominal fields\n",
    "house_cat_nom =  ['MSSubClass','MSZoning',  'LandContour', 'LotConfig', 'Neighborhood', 'Condition1' ,'BldgType', 'RoofStyle',  'Exterior1st', 'Exterior2nd', 'Foundation','Electrical','PavedDrive', 'SaleType','SaleCondition']\n",
    "house_dummy = pd.get_dummies(house[house_cat_nom], drop_first=True,dtype=int)\n",
    "house_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the dummy variables to the original data set\n",
    "house = pd.concat([house,house_dummy],axis=1)\n",
    "house.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the redundant columns\n",
    "house.drop(house_cat_nom,axis=1,inplace=True)\n",
    "house.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.SalePrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "---\n",
    "### Our EDA and data preparation are completed, we will now start building model. We will create a Linear Regression model first followed by Ridge and Lasso. we will also use cross validation to make our model stronger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_train,house_test = train_test_split(house,train_size=0.7,random_state=100)\n",
    "print(house_train.shape,house_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into X and y for train\n",
    "y_train = house_train.pop('SalePrice')\n",
    "X_train = house_train\n",
    "\n",
    "# Divide into X and y for test\n",
    "y_test = house_test.pop('SalePrice')\n",
    "X_test = house_test\n",
    "\n",
    "print(y_train.shape,X_train.shape,y_test.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Instantiate an scaler object and fit transform the train data\n",
    "scaler=StandardScaler()\n",
    "house_cont = X_train.dtypes[X_train.dtypes != \"object\"].index\n",
    "X_train[house_cont]=scaler.fit_transform(X_train[house_cont])\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the glimpse of the scaled data\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform scaling on the test data using the same scaler object\n",
    "X_test[house_cont]=scaler.transform(X_test[house_cont])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection with RFE and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start with the linear regression model and RFE intial feature value as 50\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# Apply RFE to get the top 90 features\n",
    "rfe = RFE(lm, n_features_to_select=90)\n",
    "rfe = rfe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_train.columns,rfe.support_,rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features which can be excluded to make the model according to RFE\n",
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the rfe supported columns only for both train and test\n",
    "X_train_rfe1 = X_train[X_train.columns[rfe.support_]]\n",
    "X_test_rfe1 = X_test[X_test.columns[rfe.support_]]\n",
    "print(X_train_rfe1.shape,X_test_rfe1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "lm_rfe_1 = lm.fit(X_train_rfe1, y_train)\n",
    "y_test_pred = lm_rfe_1.predict(X_test_rfe1)\n",
    "\n",
    "# Check r2 score\n",
    "round(r2_score(y_test,y_test_pred),3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm  \n",
    "X_train_rfe1 = sm.add_constant(X_train_rfe1) #Adding Constant\n",
    "X_train_rfe1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that X_train_rfe1 and y_train contain only numeric data\n",
    "X_train_rfe1 = X_train_rfe1.apply(pd.to_numeric, errors='coerce')\n",
    "y_train = y_train.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Now fit the model\n",
    "lm1 = sm.OLS(y_train, X_train_rfe1).fit()   \n",
    "print(lm1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kfold cross validation through Grid Search method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will perform k-fold cv with all 90 feature variables\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = LinearRegression()\n",
    "# Set the CV scheme and the metric\n",
    "cv_scores = cross_val_score(cv_model, X_train_rfe1, y_train, cv=5, scoring='r2')\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we have 90 hyperparameters which we now need to regulerize using k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Create a cross validation scheme\n",
    "kfold = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Step 2 : Specify the hyperparameters to be tuned\n",
    "hyper_params = [{'n_features_to_select' : list(range(1,91))}]\n",
    "\n",
    "# Step 3 : perform grid search\n",
    "\n",
    "#3.1 : Create a model object\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train_rfe1, y_train)\n",
    "rfe = RFE(lm)\n",
    "    \n",
    "#3.2 : Create a grid search object\n",
    "model_cv = GridSearchCV(estimator=rfe,scoring='r2',return_train_score=True,param_grid=hyper_params,cv=kfold,verbose=1,n_jobs=-1)\n",
    "# 3.3 : fit the model\n",
    "model_cv.fit(X_train_rfe1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result in table form\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting CV results\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
    "plt.plot(cv_results['param_n_features_to_select'] , cv_results['mean_test_score'])\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('r-squared')\n",
    "plt.title(\"Optimal Number of Features\")\n",
    "plt.legend(['test score', 'train score'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression for regulerization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, \n",
    "                    9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n",
    "ridge = Ridge()\n",
    "# cross validation\n",
    "ridgeCV = GridSearchCV(ridge, param_grid=params, scoring='neg_mean_absolute_error', cv=kfold, verbose=1, n_jobs=-1,return_train_score=True)\n",
    "ridgeCV.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation results\n",
    "ridgeCV_results = pd.DataFrame(ridgeCV.cv_results_)\n",
    "ridgeCV_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets build another Ridge with the best alpha value 100\n",
    "ridge = Ridge(alpha=100)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if we make a prediction with this Ridge model\n",
    "y_train_pred = ridge.predict(X_train)\n",
    "y_test_pred = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show Metrices\n",
    "print('Train R2:',r2_score(y_train, y_train_pred))\n",
    "print('Test R2:',r2_score(y_test, y_test_pred))\n",
    "print('--------------------')\n",
    "print('Train RMSE:',np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "print('Test RMSE:',np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most important predictors as per Ridge\n",
    "ridge_coeff_df = pd.DataFrame({'column':X_train.columns,'coeff':ridge.coef_})\n",
    "ridge_coeff_df['coeff_abs'] = np.abs(ridge_coeff_df['coeff'])\n",
    "ridge_coeff_df = ridge_coeff_df.sort_values('coeff_abs',ascending=False)\n",
    "ridge_coeff_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, \n",
    "                    9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n",
    "\n",
    "lasso  =Lasso() # Create a Lasso object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search cross validation on the alpha values for Lasso\n",
    "lassoCV = GridSearchCV(estimator=lasso,param_grid=params,scoring='neg_mean_absolute_error',cv=kfold,verbose=1,n_jobs=-1,return_train_score=True)\n",
    "\n",
    "# Fit the model\n",
    "lassoCV.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyper parameter value\n",
    "lassoCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients from Lasso\n",
    "lassoCV_df = pd.DataFrame(lassoCV.cv_results_)\n",
    "lassoCV_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build another lasso model using the best alpha value\n",
    "alpha_optimal = 0.001\n",
    "lasso = Lasso(alpha=alpha_optimal)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using Lasso\n",
    "y_train_pred = lasso.predict(X_train)\n",
    "y_test_pred = lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices using lasso\n",
    "print('Train R2:',r2_score(y_train, y_train_pred))\n",
    "print('Test R2:',r2_score(y_test, y_test_pred))\n",
    "print('--------------------')\n",
    "print('Train RMSE:',np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "print('Test RMSE:',np.sqrt(mean_squared_error(y_test, y_test_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoCV_df= pd.DataFrame(lassoCV.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting r2 score using   laso\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(lassoCV_df['param_alpha'], lassoCV_df['mean_train_score'])\n",
    "plt.plot(lassoCV_df['param_alpha'], lassoCV_df['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most important predictors \n",
    "\n",
    "coeef_df = pd.DataFrame(list(zip(X_train.columns,lasso.coef_)),columns=['Feature','Coef'])\n",
    "\n",
    "# Sort this data frame based on the absolute values of the coefficients\n",
    "\n",
    "coeef_df.sort_values(by='Coef_abs',ascending=False,inplace=True)\n",
    "\n",
    "# select top 10 predictors based on the coefficients\n",
    "coeef_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `The variables significant in predicting the price of a house are:`\n",
    "\n",
    "So the above listed Features can be considered as the most important factors to determine the price of a house.\n",
    "\n",
    "### `How well those variables describe the price of a house?`\n",
    "\n",
    "1.   GrLivArea : Above grade (ground) living area square feet, so if the GrLivArea increases by 1 (in sq feet) price of the house will increase by .12 times.\n",
    "2.  MSZoning_RL : Residential Low Density, if the house is located in residential area with low neighbourhood density then the price will increase by .08 times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question :\n",
    "After building the model, you realised that the five most important predictor variables in the lasso model are not available in the incoming data. You will now have to create another model excluding the five most important predictor variables. Which are the five most important predictor variables now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 5 variables from Lasso\n",
    "lasso_top_5 = coeef_df.head(5)['Feature']\n",
    "\n",
    "# Drop top 5 lasso var from X_train\n",
    "X_train_del_top5 = X_train.drop(lasso_top_5,axis =1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop top 5 from test data\n",
    "X_test_del_top5 = X_test.drop(lasso_top_5,axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to create another lasso model on this new dataset created after deleting the top 5 feature variable. We will follow the same sequence of steps created for the 1st lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, \n",
    "                    9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n",
    "# Create lasso object\n",
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.001}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform GridSearcg cross validation to get the best hyper parameter value\n",
    "lasso_new_CV = GridSearchCV(estimator=lasso,param_grid=params,scoring='neg_mean_absolute_error',cv=kfold,verbose=1,n_jobs=-1,return_train_score=True)\n",
    "lasso_new_CV.fit(X_train_del_top5, y_train)\n",
    "\n",
    "# Get the best hyper parameter value\n",
    "lasso_new_CV.best_params_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build another lasso model using the best alpha value\n",
    "alpha_optimal = 0.001\n",
    "lasso = Lasso(alpha=alpha_optimal)\n",
    "lasso.fit(X_train_del_top5, y_train)\n",
    "list(zip(X_train_del_top5.columns,lasso.coef_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.9072058977142287\n",
      "Test R2: 0.8672552951576801\n",
      "--------------------\n",
      "Train RMSE: 0.12075624339915147\n",
      "Test RMSE: 0.14791311219948153\n"
     ]
    }
   ],
   "source": [
    "# predict using this lasso model and evaluate metrices\n",
    "y_test_pred = lasso_new_CV.predict(X_test_del_top5)\n",
    "y_train_pred = lasso_new_CV.predict(X_train_del_top5)\n",
    "\n",
    "# Metrices\n",
    "print('Train R2:',r2_score(y_train, y_train_pred))\n",
    "print('Test R2:',r2_score(y_test, y_test_pred))\n",
    "print('--------------------')\n",
    "print('Train RMSE:',np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "print('Test RMSE:',np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coef</th>\n",
       "      <th>Coef_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>BsmtFinSF1</td>\n",
       "      <td>0.104461</td>\n",
       "      <td>0.104461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BsmtUnfSF</td>\n",
       "      <td>0.080710</td>\n",
       "      <td>0.080710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2ndFlrSF</td>\n",
       "      <td>0.070871</td>\n",
       "      <td>0.070871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LotArea</td>\n",
       "      <td>0.045425</td>\n",
       "      <td>0.045425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>YearBuilt</td>\n",
       "      <td>0.045377</td>\n",
       "      <td>0.045377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Feature      Coef  Coef_abs\n",
       "14  BsmtFinSF1  0.104461  0.104461\n",
       "16   BsmtUnfSF  0.080710  0.080710\n",
       "19    2ndFlrSF  0.070871  0.070871\n",
       "0      LotArea  0.045425  0.045425\n",
       "5    YearBuilt  0.045377  0.045377"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 5 predictors as per this new Lasso model\n",
    "lasso_new_top5 = pd.DataFrame(list(zip(X_train_del_top5.columns,lasso.coef_)),columns=['Feature','Coef'])\n",
    "\n",
    "# Sort this data frame based on the absolute values of the coefficients\n",
    "lasso_new_top5['Coef_abs'] = np.abs(lasso_new_top5['Coef'])\n",
    "lasso_new_top5.sort_values(by='Coef_abs',ascending=False,inplace=True)\n",
    "lasso_new_top5.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
